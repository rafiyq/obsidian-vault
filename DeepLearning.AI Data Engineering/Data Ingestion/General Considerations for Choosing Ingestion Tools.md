When choosing an ingestion tool for your data systems, you should consider the characteristics of the data you're ingesting, as well as the reliability and durability of the ingestion tool.

## Characteristics of the data 

_Note: In the book “Fundamentals of data engineering”, Joe and Matt refer to the characteristics of the data as the data payload, which includes data kind (type and format), shape, size, schema and data types, and metadata._

- **Data type & structure**: You learned in course 1 that data in source systems could be structured, unstructured, or semi-structured. When deciding how to ingest data and what tool to choose, you need to understand the data type and structure (e.g. an image in PNG format) so that you can identify the appropriate ingestion tool and transformations you might need to apply later on.
- **Data volume**: For data volume, you need to consider two things:
    - Data size in bytes of the existing data that you need to ingest: _In case of batch ingestion,_ you need to consider the size of the historical data that you need to ingest. Can you ingest the entire historical data in one big chunk? Depending on the network connection between the source system and the target system, it may be possible to transfer the historical data over the network, but if you have limited bandwidth then you may need to split  the massive payload into chunks, which effectively reduces the size of the payload into smaller subsections. _In case of streaming ingestion_, you need to consider the message size.  You must ensure that the streaming ingestion tool can handle the maximum expected message size. For example, Amazon Kinesis Data Streams supports a maximum message size of 1 MB, while Kafka defaults to this maximum size but can be configured to support a maximum data size of 20 MB or more.
    - The size of the future data that you may ingest with the same pipeline: how are you expecting the data to grow? What is  the daily, monthly, or yearly growth of data?  Considering the actual and future size helps you understand how to configure your tool and what cost to anticipate to ensure that your ingestion system meets the demands.
- **Latency requirements:** When designing your pipeline, one of the stakeholder requirements that you need to consider is latency: how fast do stakeholders want to operate on data? What is the acceptable delay? Do they need to extract insights from the data one day after it is ingested, or do they need near real time insights? In other words, is it a batch scenario: where data needs to be ingested once a day, a week, a month. Or does the data need to be streamed from a streaming source continuously with the lowest delay possible (for instance, in milli-seconds)? To meet the latency requirement, you need to think about how quickly you need to process the ingested data once it reaches your pipeline and also understand how quickly the source data is generated. The velocity of the data will impact the tools (batch or streaming tools) you choose to ingest and process the data.
- **Data quality**: Is the source data in good shape for immediate downstream use? What post-processing is required to serve it? Depending on the source systems the data might be incomplete, or contain inconsistent information, duplicates, or errors. If the data is not expected to be in good shape, then you may need to check the quality of the data ingested in order to fix any issues. Some ingestion tools can help you fill in missing values or detect/fix inconsistencies or invalid entries. You’ll learn more about quality checks in the upcoming course. 
- **Changes in schema**: schema changes (e.g. adding a new column, changing a column type, creating a new table, renaming a column) frequently occur in source systems and are generally out of your control. If you’re expecting these changes to happen frequently, then you might need to consider using ingestion tools that automatically detect schema changes. However, communication between you and the upstream stakeholders is as important as the automation that checks for schema changes.

## Reliability and Durability

Reliability and durability are two important considerations in the ingestion stage. Reliability means making sure that ingestion systems are performing their intended function properly. Durability means making sure that data isn’t lost or corrupted. If you design a reliable ingestion system, you will ensure the durability of the ingested data. For example, streaming systems such as IoT devices do not retain events indefinitely, so if you don’t correctly ingest its data, the data may be lost. Make sure to understand the characteristics of the source systems and the ingestion tools. 

**Advice**: Evaluate the tradeoffs between the cost of losing data vs building an appropriate level of redundancy. For more information and consideration, please check out chapter 7 of _Fundamentals of Data Engineering_.