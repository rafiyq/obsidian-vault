In the previous course, you implemented a streaming pipeline that looked like this with a Kinesis Data Firehose and S3 bucket. You were provided with a Kinesis Data Stream that streams online user activities as events or records. You process these records to compute the product recommendations and use the Data Firehose instance to deliver the records to the S3 bucket in your pipeline. In the upcoming lab, you'll learn more about how you can continue streaming these records in your pipeline and further explore Kinesis Data Streams as a source. The lab consists of two parts. In the first part, you will work with a Kinesis Data Stream acting as a router between a simple producer and a simple consumer. In the second part, you'll work again with the Course 1 scenario. You'll be provided with a Kinesis Data Stream as a source, but this time, you will use another two Kinesis Data Streams to continue streaming the records in your pipeline. From each of these two new Data Streams, data will be taken by a Data Firehose and delivered to the respective S3 bucket. Let's go through the first part of this lab. To gain a better understanding of the components of an event streaming platform, you will first create a Kinesis Data Stream and then interact with it from the producer and consumer sides. For that, you're provided with two Python scripts, consumer from CLI and producer from CLI. The producer script represents a simple producer application that writes a single data record into the Kinesis Data Stream. The data record is a JSON string that contains the details of a user session, such as the session ID, customer number, city, country, and the browsing history. The producer's Python script uses Boto3 to interact with Kinesis. You can run the script from the terminal, which expects two arguments, the name of the Kinesis Data Stream and the JSON string of a single record. In the producer script, these two arguments are passed into the Boto3 method putRecord to write the record into the Kinesis Data Stream. The consumer script also represents a simple consumer application that you can run from the terminal by specifying the name of the Kinesis Data Stream. When you run the consumer script, it'll iterate through all the shards of the data stream, extract all records from each shard, and then print some information in the terminal about each record. If you check the code of the consumer script, you can see that Boto3 is also used. In the function pullShards, the consumer continuously iterates through the shards of the data stream, edits the records using the Boto3 method getRecords, and then prints this text in the terminal to explain which record was read from which shard and at what position within the shard. In the first part of the lab, you won't edit the producer and consumer scripts, but you will be tasked to run these scripts. After you create the Kinesis Data Stream, you'll first run the consumer script in the terminal. To do that, you'll first activate the JupyterLab environment, then navigate to the folder sourceCLI or srcCLI, and finally run the consumer script as shown here by providing it the name of the Kinesis stream you created. You'll notice that nothing will be printed in the terminal because the data stream is now empty. You'll keep this terminal open to keep the consumer running, and then in another terminal, you'll run the producer script to write a record in the data stream. Again, in the new terminal, you'll navigate to the folder sourceCLI or srcCLI, and then execute the producer script by specifying the name of the Kinesis stream and the JSON string representing the record. So now, if you check the first terminal where the consumer is running, you'll see that the consumer has read the record that you just sent to the data stream. After you're done with the first part of the lab, you'll get back to the original ecommerce scenario. You'll be provided with a Kinesis Data Stream that represents your source system, so you'll be at the consumer side ingesting data from the streaming source. You'll implement a streaming ETL pipeline, where you'll first apply a simple transformation on the ingested records, and then continue streaming these records in your pipeline. For that, you'll set up two Kinesis Data Streams. You'll send the records that correspond to USA customers to one data stream, and those that correspond to international customers to another data stream. This is assuming that your company noticed that customers showed different purchase behaviors based on their countries. So if they're located in the USA, their online activities need to be processed by a certain recommendation engine. Otherwise, their online activities need to be processed by another recommendation engine. For each of these two data streams, data will be automatically taken by a data firehose and delivered to their respective S3 bucket. You will first create the two data streams, the two firehose instances, and the two buckets using Boto3. Then you'll write the transformation code in the consumer script that is provided to you under this ETL folder. This script can also be run from the terminal, and it expects the name of the source data stream and the names of the two destination data streams. The script contains this pull shards function, where you're provided with the code that iterates through the shards to extract the records. Here, once a record is extracted, you will need to complete this part of the code to transform the record. The transformation consists of adding three fields, as shown here. The first field, overall product quantity, represents the sum of the product quantities that are listed in the browsing history. The second field, overall in shopping cart, represents the sum of the product quantities for the products that are placed in the shopping cart. And the third field, total different products, represents the number of products that are listed in browse history. And finally, you will send the transform record to the corresponding data stream based on the value of the country field. After you apply these modifications to the consumer script, you will run the script in the terminal by specifying the name of the source data stream and the names of the two destination data streams, as shown here. When you run this command, the consumer script will read the records from the source data stream, transform them, and then send them to the corresponding Kinesis streams. The Kinesis firehose indices will automatically deliver the data to the S3 buckets. So that was an overview of the tasks you'll perform in this lab, and now you're ready to get started. After you finish the lab, join me back here for a quick summary of this week.